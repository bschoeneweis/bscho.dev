---
title: 'Building an inverted index from scratch in Rust'
date: '2025-10-05'
tags: ['rust', 'search']
description: 'Creating a simple inverted index from scratch in Rust, from tokenization to querying.'
---

TODO:

Recently at [Radar](https://radar.com), I've been working on search (forward geocoding / autocomplete for addresses, POIs, and regions), and writing lots of Rust.

In order to dive a bit deeper into search fundamentals, I thought it would be interesting to dive into one of the most fundamental data structures in search, an [inverted index](https://en.wikipedia.org/wiki/Inverted_index) (sometimes referred to as a Posting List).

## A quick primer
An [inverted index](https://en.wikipedia.org/wiki/Inverted_index) is a fundamental data structure in the search world, powering text-based search for libraries like [Apache Lucene](https://lucene.apache.org/) and [Tantivy](https://github.com/quickwit-oss/tantivy).

To illustrate an inverted index, it's helpful to contrast with a [forward index](https://en.wikipedia.org/wiki/Search_engine_indexing#The_forward_index).

### Forward index

A forward index maps documents to the set of terms they contain:

| Document | Terms                                            |
| -------- | ------------------------------------------------ |
| 1        | the lord of the rings the fellowship of the ring |
| 2        | the lord of the rings the two towers             |
| 3        | the lord of the rings the return of the king     |
| ...      | ...                                              |
| 10       | star wars episode vi return of the jedi          |

Given this structure, you could imagine if we had a full catalog of movies (and maybe books as well), a query to find titles in a traditional forward index might look something like:

```sql
SELECT * FROM my_media WHERE title LIKE '%return%';
```

This query requires scanning over every document in the index and checking if the term exists.  This can quickly get out of hand as datasets grow and grow (by row count, but also my term per document).

### Inverted index

An inverted index maps a set of terms to the documents they appear in:

| Term       | Documents   |
| ---------- | ----------- |
| episode    | 10          |
| fellowship | 1           |
| iv         | 10          |
| jedi       | 10          |
| king       | 3           |
| lord       | 1, 2, 3     |
| of         | 1, 2, 3, 10 |
| return     | 3, 10       |
| ring       | 1           |
| rings      | 1, 2, 3     |
| star       | 10          |
| the        | 1, 2, 3, 10 |
| towers     | 2           |
| two        | 2           |
| wars       | 10          |


Here we see each term mapped to the respective documents in which they appear.  Also notice that the index is sorted alphabetically by term (a small optimization).

The set of matching documents (the structure in the right column) here is often called a **posting list** (or postings).

The other commonly associated structure with an inverted index is the **dictionary**.

The dictionary is the unique set of the terms that exists in the index.  It contains pointers from each term to its respective posting list, for quick lookups.  Since the posting lists can be quite large, dictionaries are often kept in memory and point to the location of the posting list on disk.

Additionaly, metadata such as **document frequency**, which tells us how many documents contain each term (also equal to the length of the posting list), may also be stored on the dictionary.  Document frequency can be helpful for ranking our documents after retrieval, which we will discuss more in a bit.

#### Searching
Instead of the full index scan that needs to occur with the forward index, we can now do a term-lookup in approximately constant time.  These faster lookups generally come at the cost of slower write speeds given the complexity of the indexing (we'll dive into this later), but the faster reads generally outweight the costs here.

For example, we can search for `"return"` and see that documents `3` and `10` have a match and return those documents.  This reduced our operation count from scanning `n` rows to reading the index and operating on the matching documents (which will almost always be much less than the total indexed rows).

#### Filtering
Since the result of the lookup is document ids (or the posting list), we can also take advantage of set operations (`AND`, `OR`, `NOT`) to introduce more powerful querying paradigms, such as filtering.

For example, we can support queries like `"rings" AND "return"` by doing lookups for both terms, and then taking the intersection of the matching documents: `[1, 2, 3] ∩ [3, 10] = [3]`.

We can do the same for `OR` queries: `"rings" OR "fellowship"` by taking the union: `[1, 2, 3] ∪ [1] = [1, 2, 3]`.

`NOT` queries can be supported by taking the set difference of _all_ of the document ids and the term posting list.  For example: `NOT "return"`: `[1, 2, 3, 10] \ [3, 10] = [1, 2]`.

#### Ranking
The last topic to briefly cover before jumping into some code is ranking.  The lookups using our structure above help us to determine which documents match given a term(s), but it doesn't tell us _how well_ they match with respect to the rest of our data or help us pick certain documents over another when retrieving.

---

**A quick aside**

Two fundamental terms in search are **recall** and **precision** (more in-depth definitions [here](https://en.wikipedia.org/wiki/Precision_and_recall)).

tl;dr is that:
- **Recall** is the proportion of relevant documents that were successfully retrieved.  Said otherwise, did we find all the documents that matter?
- **Precision** is the proportion of retrieved documents that are actually relevant.  Said otherwise, are the documents we returned correct?

High recall means finding most of relevant documents, even if some bad or irrelevant ones slip through.

High precision means your top results are highly relevant, even if you fail to fetch a few.

Balancing recall and precision is an art, but generally search systems aim to _retrieve_ results with high recall, and _rank_ by relevance to improve precision.

---

One of the most fundamental ranking algorithms used in search is [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf), or **Term Frequency-Inverse Document Frequency**.

This algorithm (while relatively simple) tries to tell us how important a term is in a document, across a collection or index of documents (also called a corpus).  The mathematics can be seen in the link above, but to provide a quick understanding, we can look at TF-IDF in two parts.

The first part of the algorithm is the **Term Frequency**.  This is a measurement of how often a term appears within a document.  Generally, the more often a term appears, the more important it is.

The second part is the **Inverse Document Frequency**.  This is a measurement of how often a term appears across the entire collection of documents.  In contrast, the _less_ a term appears across the corpus, the more significant it is.

---

For example, we can calculate the TF-IDF for two terms: `rings` and `fellowship` across a corpus of size `N = 4`.

The posting list for `rings` is `[1, 2, 3]`.  Given the occurences, we can calculate the Term Frequency (TF):

| Document | Term Frequency (TF) | Total Terms | Normalized TF |
| -------- | -------------- | ----------- | ------------- |
| 1        | 1              | 9           | 1/9 ≈ 0.111   |
| 2        | 1              | 7           | 1/7 ≈ 0.143   |
| 3        | 1              | 8           | 1/8 = 0.125   |

Followed by the Inverse Document Frequency (IDF):

`IDF = log10(N / nₜ) = log10(4 / 3) ≈ 0.125`

Lastly, we do TF x IDF:

| Document | TF    | IDF   | TF-IDF |
| -------- | ----- | ----- | ------ |
| 1        | 0.111 | 0.125 | 0.0139 |
| 2        | 0.143 | 0.125 | 0.0179 |
| 3        | 0.125 | 0.125 | 0.0156 |

What exactly do these values mean?  Because the TF-IDF is smaller, than means the term `rings` (given our corpus), does not help us distinguish much between the documents.

Now let's contrast that with the more unique term, `fellowship`.

The posting list for `fellowship` is `[1]`.  With only a single occurence, we can still calculate the Term Frequency (TF):

| Document | Term Frequency (TF) | Total Terms | Normalized TF |
| -------- | -------------- | ----------- | ------------- |
| 1        | 1              | 9           | 1/9 ≈ 0.111   |

With the following IDF:

`IDF = log10(N / nₜ) = log10(4 / 1) ≈ 0.602`

Finishing with TF x IDF:

| Document | TF    | IDF   | TF-IDF |
| -------- | ----- | ----- | ------ |
| 1        | 0.111 | 0.602 | 0.0669 |

Even though these two terms have the same term frequency (primarily because the per-document text is small), `fellowship` has a much higher TF-IDF score, showing it's significance in _relevance_ between results.

Another common (and more evolved) ranking algorithm is [BM25](https://en.wikipedia.org/wiki/Okapi_BM25).  I won't cover it much here other than to say it can address some shortcomings with TF-IDF such as normalizing document length (you have one document that is significantly longer than another), and more.  BM25 is the more commonly used ranking algorithm in systems like Lucene and Tantivy.

---

To summarize the basics, with an inverted index plus some math, we can now:

- Retrieve documents efficiently by term
- Apply filters using set operations (`AND`, `OR`, `NOT`)
- Rank them by relevance (TF-IDF)


Now we can dive into a simple implementation to see what this looks like!

## Building the index

### Tokenization

### The data structure

### Querying

### Improve performance

Sorted posting lists → enables binary search for intersections instead of contains().

Compression → delta encoding to reduce memory.

Multi-term queries → merge iterators instead of cloning vectors.

Stop words → ignore common words like “the”, “a”, “is”.