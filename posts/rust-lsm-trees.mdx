---
title: 'A simple log-structured merge tree in Rust'
date: '2025-10-22'
tags: ['rust', 'search']
description: 'Building a simple log-structured merge tree (LSM tree) from scratch in Rust.'
---

> This is an attempt at a high-level introduction to the architecture of LSM trees.  Certain implementation details or features (there are a lot!) may not be covered.  If you're interested in LSM trees in detail, a great resource is the [RocksDB wiki](https://github.com/facebook/rocksdb/wiki/RocksDB-Overview).

A [log-structured merge tree](https://en.wikipedia.org/wiki/Log-structured_merge-tree) (or an LSM tree) is a write-optimized data structure that is used in databases like [RocksDB](https://rocksdb.org/) and [Cassandra](https://cassandra.apache.org/_/index.html).

By buffering writes in memory, and then flushing sorted files to disk, LSM trees can achieve high write throughput and efficient reads to power use-cases like [observability](https://greptime.com/), [streaming](https://risingwave.com/), and [more](https://tigerbeetle.com/).

## Fundamentals

There are a few fundamental pieces to chat through before we jump into an implementation.

The write path of LSM trees does a good job showcasing how each core component fits together, so we'll first cover what happens when a write comes in.  Then, we'll briefly cover the read path to complete the picture.

### Writes

```plaintext
             ┌─────────┐
             │  Write  │
             └────┬────┘
                  ▼
        ┌─────────┴──────────┐
        ▼                    ▼
  ┌─────┴─────┐       ┌──────┴──────┐
  │    WAL    │       │  Memtable   │
  │ (on disk) │       │ (in memory) │
  └───────────┘       └──────┬──────┘
                             │ flush
                  ┌──────────┘
                  ▼
      ┌───────────┴────────────┐
      │ SSTable1  SSTable2 ... │ (on disk)
      └───────────┬────────────┘
                  │ compact
                  ▼
           merged SSTable
```

When a write comes in, it is simultaneously written to a memtable and a [write-ahead log](https://en.wikipedia.org/wiki/Write-ahead_logging) (WAL).

#### Memtable
The memtable is an in-memory buffer that temporarily holds all incoming writes (before they get persisted to disk).  Because writing to memory is [orders of magnitude faster](https://github.com/sirupsen/napkin-math?tab=readme-ov-file#numbers) than writing to disk, we can quickly acknowledge the writes.

Memtables are usually built from [skip lists](https://en.wikipedia.org/wiki/Skip_list) or [red-black trees](https://en.wikipedia.org/wiki/Red%E2%80%93black_tree), which store sorted key-value pairs (sorted by key).  The sorted layout gives us the benefit of efficient lookups, and speeds up future on-disk merges.

Once a certain size limit is hit, the memtable gets saved to disk.  While being saved to disk, the memtable being flushed will become immutable (although still queryable), and another memtable will take over incoming writes.

#### Write-ahead log (WAL)
Given that the memtable lives in-memory, we also want to provide durability to our system.  Enter, the WAL.

The WAL is an append-only file that gives us data durability.  Without the WAL, if the system were to crash, we would lose all of the data in the memtable that hasn't yet been flushed to disk (no good!).  With the WAL, we can replay all of the operations in the log file to reconstruct the last state of the memtable.

Once the memtable is safely flushed to disk (up next) and the data is persisted, the corresponding WAL entries can be purged.

#### Sorted string tables (SSTables)
As previously described, once the memtable reaches its size limit and "gets flushed", it gets saved to a Sorted String Table, or SSTable.

An SSTable is an immutable, sorted file that stores key-value pairs (ordered by key).  These files usually also contain index blocks to quickly locate keys within the file, and [bloom filters](https://en.wikipedia.org/wiki/Bloom_filter) to filter through files that don't contain a given key.

One important performance aspect here is that the SSTables are writen using sequential I/O, meaning we batch the data in large, contiguous chunks on disk.  This makes our writes (and reads which we'll cover soon) faster than random I/O.  This also helps us fully utilize the disk throughput when flushing (or when compacting, which we'll cover next).

Because SSTables are immutable, each memtable flush creates a new SSTable.  Over time, these SSTables accumulate, often containing different versions of keys.

#### Compaction
Periodically, we take the accumulated SSTables, and we _compact_ and consolidate them into larger SSTables.

Compaction keeps data sorted, removes outdated versions and deleted keys, and reduces the number of files that must be searched during reads.  It's an important process for maintaining stable performance as data grows.

There are several compaction algorithms, but we'll briefly look at two of the most common: leveled compaction and size-tiered compaction.

##### Leveled compaction
Used by RocksDB, leveled compaction organizes SSTables into different levels (L0, L1, L2, ...).  Each level has a maximum size, and after L0, SSTables within the same level are guaranteed to have no overlapping key ranges.

New SSTables are written to L0, so that initial non-overlapping guarantee can't hold.  But, once the size limit of L0 is hit, the files get merged into the next level (each is ~10x bigger than the previous) with non-overlapping keys.

The result is strong read performance.  To find a key, only one SSTable needs to be looked at per level.

This comes at the cost of high [write amplification](https://en.wikipedia.org/wiki/Write_amplification) due to all of the small merging operations as data falls through the levels.

##### Size-Tiered compaction
Used by Cassandra, size-tiered compaction groups SSTables of similar sizes into tiers.  Once there are enough SSTables of roughly the same size within a tier, they get merged into a single, larger SSTable and moved to the next tier.

The result is high write throughput due to the lower write amplification (fewer compactions).

The tradeoff is higher read amplification, since a key might appear in multiple overlapping SSTables.

### Reads

```plaintext

                ┌────────┐
                │  Read  │
                └───┬────┘
                    ▼
    ┌──────────────────────────────────┐
    │  Memtable (in memory)            │
    │  Immutable Memtable (flushing)   │
    └───────────────┬──────────────────┘
                    │ (not found in memory)
                    ▼
                    │ (check L0 bloom filter)
                    ▼
  L0 (on disk) - - - - - - - - - - - - - -
        ┌────────┐     ┌────────┐
        │   L0   │     │   L0   │
        │ [SSTs] │     │ [SSTs] │
        └────────┘     └────────┘
            (return if found)
                    ┬
                    │ (check L1 bloom filter)
                    ▼
  L1 (on disk) - - - - - - - - - - - - - -
  ┌───────────────┐   ┌───────────────┐
  │      L1       │   │      L1       │
  │    [SSTs]     │   │    [SSTs]     │
  └───────────────┘   └───────────────┘
            (return if found)
                   ...
                    ┬
                    │ (check LN bloom filter)
                    ▼
  LN (on disk) - - - - - - - - - - - - - -
  ┌───────────────────────────────────┐
  │                 LN                │
  │               [SSTs]              │
  └───────────────────────────────────┘

```

Now that we've covered the high-level pieces on the write side, we can outline what an incoming read looks like.

The main idea for reads is that we search using a layered approach since we know that our most recent writes will be in-memory, and then they move to disk progressing through older and larger SSTables.

#### Step 1: Check the memtable
Our first check should be seeing if our key lives in memory.  The memtable will hold our most recent writes, and we'll avoid disk I/O if we have a match.  If we're in the middle of a flush, we can also check the immutable memtable being flushed as that is still queryable.

#### Step 2: Bloom filters
Before scanning through SSTables on disk, we use bloom filters to rule out any files that _cannot_ contain the key.

The tl;dr with bloom filters is that they either tell you that a key _probably exists_ in a set (false positives **are** possible here), or that a key does not exist in a set (false negatives are **not** possible).  This check is key in helping to minimize unnecessary disk I/O, and we can leverage it to skip levels entirely.

#### Step 3: SSTables
With some candidates filtered out, we search through the remaining SSTables in order from newest to oldest (newer tables have the newer writes), level by level.

Because each SSTable is sorted by key, and having index blocks with each SSTable, we can achieve fairly efficient reads.  We can also take advantage of sequential I/O during scans because the tables were stored contiguously.

## Implementation

Now with some background, let's write out some of the basic pieces we described above.  This is meant to be a _very_ simple implementation just to illustrate key concepts, so we'll skip over some details and optimizations.

Our goal here is to achieve a basic structure that looks like this:

```rust
pub struct LsmTree {
    memtable: Memtable,
    wal: Wal,
    sstables: Vec<SSTable>,
}
```

and we'll implement the following operations:

```rust
impl LsmTree {
    const MAX_ENTRY_SIZE: usize = 64 * 1024; // 64 KB

    pub fn put(&mut self, key: Vec<u8>, value: Vec<u8>) -> std::io::Result<()>;
    pub fn get(&self, key: &[u8]) -> std::io::Result<Option<Vec<u8>>>;
    pub fn flush(&mut self) -> std::io::Result<()>;
}
```

Since we already have a lot of code to write, I'm going to skip over a few things like:
- Deleting records (by adding tombstones, you can read more [here](https://disc-projects.bu.edu/lethe/#:~:text=Deletes%20in%20LSM%2Dtrees%20are,the%20physical%20target%20data%20entries.))
- Index blocks for reads
- Bloom filters for reads

One other note is I'll be adding [anyhow](https://docs.rs/anyhow/latest/anyhow/) to simplify error handling (can be added with `cargo add anyhow`).

Let's get started!

### Memtable
We'll start with the memtable.  Above we described that they typically use skip lists or red-black trees, to keep things simple we're going to use a [BTreeMap](https://doc.rust-lang.org/std/collections/struct.BTreeMap.html), which will still keep our memtable ordered by key.

```rust
use std::collections::BTreeMap;

pub struct Memtable {
    map: BTreeMap<Vec<u8>, Vec<u8>>,
}

impl Memtable {
    pub fn new() -> Self {
        Self {
            map: BTreeMap::new(),
        }
    }

    pub fn put(&mut self, key: Vec<u8>, value: Vec<u8>) {
        self.map.insert(key, value);
    }

    pub fn get(&self, key: &[u8]) -> Option<&[u8]> {
        self.map.get(key).map(|v| v.as_slice())
    }

    pub fn iter(&self) -> impl Iterator<Item = (&[u8], &[u8])> {
        self.map
            .iter()
            .map(|(k, v)| (k.as_slice(), v.as_slice()))
    }

    pub fn size(&self) -> usize {
        self.map.len()
    }

    pub fn is_empty(&self) -> bool {
        self.map.is_empty()
    }

    pub fn clear(&mut self) {
        self.map.clear();
    }
}
```

Nothing crazy here as this is largely just a wrapper over built-in `BTreeMap` operations, but it will help us build upon the implementation in a bit.

### WAL
Next, we need some durability so we can sleep easy at night knowing our data is on-disk.

A few small notes about the implementation here:
- We're just going to follow a very basic log scheme of `<u32 key length><u32 value length><key bytes><val bytes>`
- No deletes

```rust
use std::{
    fs::{File, OpenOptions},
    io::{BufReader, BufWriter, Read, Write},
    path::{Path, PathBuf},
};

use anyhow::{anyhow, bail, Context, Result};

fn read_entry_from_header(header: &[u8; 8]) -> Result<(Vec<u8>, Vec<u8>)> {
    let key_length = u32::from_le_bytes(
        header_buffer
            .get(0..4)
            .ok_or_else(|| anyhow!("Invalid header: missing key length"))?
            .try_into()
            .context("Invalid key length slice")?,
    ) as usize;

    let value_length = u32::from_le_bytes(
        header_buffer
            .get(4..8)
            .ok_or_else(|| anyhow!("Invalid header: missing value length"))?
            .try_into()
            .context("Invalid value length slice")?,
    ) as usize;

    // defensive check to avoid any OOM even though we also check on write
    if key_length > LsmTree::MAX_ENTRY_SIZE || value_length > LsmTree::MAX_ENTRY_SIZE {
        bail!(
            "Corrupt entry: header too large (key length={} value length={})",
            key_length,
            value_length
        )
    }

    let mut key = vec![0u8; key_length];
    reader.read_exact(&mut key)?;

    let mut value = vec![0u8; value_length];
    reader.read_exact(&mut value)?;

    Ok((key, value))
}

pub struct Wal {
    path: PathBuf,
    writer: BufWriter<File>,
}

impl Wal {
    pub fn open(path: &Path) -> Result<Self> {
        let path = path.to_path_buf();
        let file = OpenOptions::new()
            .create(true)
            .append(true)
            .read(true)
            .open(&path)?;

        Ok(Self {
            path,
            writer: BufWriter::new(file),
        })
    }

    /// Append a `Put(key, value)` record to the WAL and fsync
    /// using the following log format:
    /// `<u32 key length><u32 value length><key bytes><val bytes>`
    pub fn append(&mut self, key: &[u8], value: &[u8]) -> Result<()> {
        let key_length = key.len() as u32;
        let value_length = value.len() as u32;

        self.writer.write_all(&key_length.to_le_bytes())?;
        self.writer.write_all(&value_length.to_le_bytes())?;
        self.writer.write_all(key)?;
        self.writer.write_all(value)?;

        // flush buffer and sync the file for durability
        self.writer.flush()?;
        self.writer.get_ref().sync_all()?;

        Ok(())
    }

    /// Replay all record in the WAL (returns key/value pairs to rebuild the memtable)
    pub fn replay(&mut self) -> Result<Vec<(Vec<u8>, Vec<u8>)>> {
        // flush our any buffered writes
        self.writer.flush()?;

        let file = OpenOptions::new().read(true).open(&self.path)?;
        let mut reader = BufReader::new(file);

        let mut records = Vec::new();
        let mut header_buffer = [0u8; 8];

        loop {
            match reader.read_exact(&mut header_buffer) {
                Ok(()) => {}
                Err(e) if e.kind() == std::io::ErrorKind::UnexpectedEof => break,
                Err(e) => return Err(e).context("Failed to read WAL header"),
            }

            let (key, value) = read_entry_from_header(&header_buffer)?;
            records.push((key, value));
        }

        Ok(records)
    }

    /// Purges the WAL after a flush
    pub fn purge(&mut self) -> Result<()> {
        // truncate then re-init
        OpenOptions::new()
            .create(true)
            .write(true)
            .truncate(true)
            .open(&self.path)?;

        let file = OpenOptions::new()
            .append(true)
            .read(true)
            .open(&self.path)?;

        self.writer = BufWriter::new(file);

        Ok(())
    }
}
```

### SSTables
We'll also make a few simplifying assumptions for the SSTable implementation.
- One file per flush
- File is sorted by entry key

```rust
pub struct SSTable {
    path: PathBuf,
}

impl SSTable {
    /// Creates an SSTable file from the data in a memtable.
    /// Format of the entries remains the same.
    pub fn from_memtable(path: &Path, memtable: &Memtable) -> Result<Self> {
        let path_buf = path.to_path_buf();
        let mut file = File::create(&path)?;

        // write all of the pre-sorted data
        for (key, value) in memtable.iter() {
            let key_length = key.len() as u32;
            let value_length = value.len() as u32;
            file.write_all(&key_length.to_le_bytes())?;
            file.write_all(&value_length.to_le_bytes())?;
            file.write_all(key)?;
            file.write_all(value)?;
        }

        file.flush()?;
        file.sync_all()?;

        Ok(Self { path: path_buf })
    }

    /// Find a single key on-disk.
    /// Sequentially scans each record until a match is found or we EOF.
    pub fn get(&self, target_key: &[u8]) -> Result<Option<Vec<u8>>> {
        let file = File::open(&self.path)?;
        let mut reader = BufReader::new(file);

        let mut header_buffer = [0u8; 8];
        loop {
            match reader.read_exact(&mut header_buffer) {
                Ok(()) => {}
                Err(e) if e.kind() == std::io::ErrorKind::UnexpectedEof => break,
                Err(e) => return Err(e).context("Failed to read SSTable header"),
            }

            let (key, value) = read_entry_from_header(&header_buffer)?;
            if target_key == key {
                return Ok(Some(value));
            }
        }

        Ok(None)
    }

    /// Simple iterator for convenience to go over all key/values
    /// in an SSTable (primarily for compaction)
    pub fn iter(&self) -> Result<impl Iterator<Item = Result<(Vec<u8>, Vec<u8>)>>> {
        let file = File::open(&self.path)?;
        let mut reader = BufReader::new(file);

        let mut header_buffer = [0u8; 8];

        Ok(std::iter::from_fn(move || {
            match reader.read_exact(&mut header_buffer) {
                Ok(()) => {}
                Err(e) if e.kind() == std::io::ErrorKind::UnexpectedEof => return None,
                Err(e) => return Some(Err(e).context("Failed to read SSTable header")),
            }

            match read_entry_from_header(&header_buffer) {
                Ok((key, value)) => Some(Ok((key, value))),
                Err(e) => Some(Err(e)),
            }
        }))
    }
}
```

---

## Resources
There are some links scattered throughout, but here are some useful resources that helped guide some of the writing above:
- [RocksDB Overview](https://github.com/facebook/rocksdb/wiki/RocksDB-Overview)
- [ScyllaDB Glossary](https://www.scylladb.com/glossary)
- [The Secret Sauce Behind NoSQL: LSM Tree](https://www.youtube.com/watch?v=I6jB0nM9SKU)
