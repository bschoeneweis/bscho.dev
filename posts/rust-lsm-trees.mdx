---
title: 'A simple log-structured merge tree in Rust'
date: '2025-10-22'
tags: ['rust', 'search']
description: 'Building a simple log-structured merge tree (LSM tree) from scratch in Rust.'
---

A [log-structured merge tree](https://en.wikipedia.org/wiki/Log-structured_merge-tree) (or an LSM tree) is a write-optimized data structure that is used in databases like [RocksDB](https://rocksdb.org/) and [Cassandra](https://cassandra.apache.org/_/index.html).

By buffering writes in memory, and then flushing sorted files to disk, LSM trees can achieve high write throughput and efficient reads to power use-cases like [observability](https://greptime.com/), [streaming](https://risingwave.com/), and [more](https://tigerbeetle.com/).

## Fundamentals

There are a few fundamental pieces to chat through before we jump into an implementation.  First, we'll talk about the building blogs that make up the write path.

### Writes

```plaintext
             ┌─────────┐
             │  Write  │
             └────┬────┘
                  ▼
        ┌─────────┴──────────┐
        ▼                    ▼
  ┌─────┴─────┐       ┌──────┴──────┐
  │    WAL    │       │  Memtable   │
  │ (on disk) │       │ (in memory) │
  └───────────┘       └──────┬──────┘
                             │ flush
                  ┌──────────┘
                  ▼
      ┌───────────┴────────────┐
      │ SSTable1  SSTable2 ... │ (on disk)
      └───────────┬────────────┘
                  │ compact
                  ▼
           merged SSTable
```

When a write comes in, it is simultaneously written to a [write-ahead log](https://en.wikipedia.org/wiki/Write-ahead_logging) (WAL) and a memtable.

#### Memtable
The memtable is an in-memory buffer where all writes initially land (because writing to memory is [_much_ faster](https://github.com/sirupsen/napkin-math?tab=readme-ov-file#numbers) than writing to disk).  Memtables are usually built from [skip lists](https://en.wikipedia.org/wiki/Skip_list) or [red-black trees](https://en.wikipedia.org/wiki/Red%E2%80%93black_tree), and store sorted key-value pairs (sorted by key).  The keys are sorted up at this stage to make later on-disk merges and lookups more efficient.

Once a certain size limit is hit, the memtable gets saved to disk.

#### Write-ahead log (WAL)
Given that the memtable lives in-memory, we also want to provide durability to our system.  Enter, the WAL.

The WAL is an append-only file that gives us data durability.  Without the WAL, if the system were to crash, we would lose all of the data in the memtable that hasn't yet been flushed to disk (no good!).  With the WAL, we can replay all of the operations in the log file to reconstruct the last state of the memtable.

Once the memtable is safely flushed to disk (up next) and the data is persisted, the corresponding WAL entries can be purged.

#### Sorted string tables (SSTables)
Once the memtable reaches its size limit and "gets flushed", it gets saved to a Sorted String Table, or SSTable.

An SSTable is an immutable, sorted file that stores key-value pairs (ordered by key).  These files usually also contain index blocks to quickly locate keys within the file, and bloom filters to filter through files that don't contain a given key.

Because SSTables are immutable, each memtable flush creates a new SSTable.  Over time, these SSTables accumulate, often containing different versions of keys.

#### Compaction
Periodically, we take the accumulated SSTables, and we _compact_ and consolidate them into larged SSTables.

Compaction keeps data sorted, removes outdated versions and deleted keys, and reduces the number of files that must be searched during reads.  It's an important process for maintaining stable performance as data grows.

There are several compaction algorithms, but we'll briefly look at two of the most common: leveled compaction and size-tiered compaction.

##### Leveled compaction
Used by RocksDB, leveled compaction organizes SSTables into different levels (L0, L1, L2, ...).  Each level has a maximum size, and after L0, SSTables within the same level are guaranteed to have no overlapping key ranges.

New SSTables are written to L0, so that initial non-overlapping guarantee can't hold.  But, once the size limit of L0 is hit, the files get merged into the next level (each is ~10x bigger than the previous) with non-overlapping keys.

The result is strong read performance.  To find a key, only one SSTable needs to be looked at per level. This of course comes at the cost of high [write amplification](https://en.wikipedia.org/wiki/Write_amplification) due to all of the small merging operations as data falls through the levels.

##### Size-Tiered compaction
Used by Cassandra, size-tiered compaction groups SSTables of similar sizes into tiers.  Once there are enough SSTables of roughly the same size within a tier, they get merged into a single, larger SSTable and moved to the next tier.

The result is lower write amplification (fewer compactions) which yields high write throughput.  The tradeoff higher read amplification, since a key might appear in multiple overlapping SSTables.

